# MoE_ViT
Implementation and FineTuning of ViT with MoE on CIFAR10

Colab notebook demonstrating the integration of MLP Mixture of Experts (MoE) with Google Vision Transformers (ViT) (vit-base-patch16-224) for image classification. The notebook includes step-by-step code to modify the ViT architecture with MoE, fine-tune the model, and evaluate its performance on the CIFAR10 dataset.

Model: https://huggingface.co/google/vit-base-patch16-224

Dataset: https://huggingface.co/datasets/uoft-cs/cifar10
