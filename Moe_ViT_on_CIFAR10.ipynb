{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOGUxBIGgwEN8stKNeusLyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitOfTheseus/MoE_ViT/blob/main/Moe_ViT_on_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation and Training of a MoE-ViT**"
      ],
      "metadata": {
        "id": "-Qdik18ExobG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements and train a Vision Transformer (ViT) endowed with Sparse Mixture of Expert (MoE) for image classification on the CIFAR10 Dataset."
      ],
      "metadata": {
        "id": "sY4wiHlKxqFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive to save results, figures and checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzTh_AkxzSfE",
        "outputId": "2a83a45f-487d-4625-83e8-7f88b1e64a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q transformers datasets triton # installing libraries not included in colab"
      ],
      "metadata": {
        "id": "dqB4DopO2wtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31592a3b-5c50-4e4e-fe02-3c6a6c55cbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from datasets import Dataset as Ds\n",
        "from triton.language import trans\n",
        "\n",
        "from transformers import ViTImageProcessor\n",
        "from transformers import ViTForImageClassification"
      ],
      "metadata": {
        "id": "1LLhtkV8-QQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting first parameters and variables\n",
        "random_seed = 0\n",
        "torch.manual_seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dataset_name = 'cifar10'\n",
        "model_name = 'MoE_ViT_on_' + dataset_name"
      ],
      "metadata": {
        "id": "lf1MakkvyET-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design of the Sparse MoE with MLPs"
      ],
      "metadata": {
        "id": "5U6b-7hIAfqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me_kC52qxfCR"
      },
      "outputs": [],
      "source": [
        "class SparseMoE(nn.Module):\n",
        "    \"\"\"Custom Class for Sparse MoE with MLP\"\"\"\n",
        "\n",
        "    def __init__(self, dim, hidden_dim, num_experts=32, top_k=2, experts=None):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.dim = dim\n",
        "\n",
        "        # Defining the experts\n",
        "        self.experts = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        ) for _ in range(num_experts)])\n",
        "\n",
        "        # Gating network\n",
        "        self.gate = nn.Linear(dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, dim)\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Calculating gating scores and selecting top-k experts\n",
        "        gate_logits = self.gate(x)  # Shape: (batch_size, seq_len, num_experts)\n",
        "        topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)  # Get top-k experts\n",
        "\n",
        "        # Normalizing gating values\n",
        "        topk_values = torch.softmax(topk_values, dim=-1)\n",
        "\n",
        "        # Flattening the batch and sequence dimensions for processing\n",
        "        x_flat = x.view(-1, dim)  # Shape: (batch_size * seq_len, dim)\n",
        "        topk_indices_flat = topk_indices.view(-1, self.top_k)  # Shape: (batch_size * seq_len, top_k)\n",
        "        topk_values_flat = topk_values.view(-1, self.top_k)  # Shape: (batch_size * seq_len, top_k)\n",
        "\n",
        "        # Initializing output tensor\n",
        "        output = torch.zeros_like(x_flat)\n",
        "\n",
        "        # Applying each expert and aggregating outputs\n",
        "        for i in range(self.top_k):\n",
        "            expert_idx = topk_indices_flat[:, i]  # Indices of selected experts for each token\n",
        "            expert_weight = topk_values_flat[:, i].unsqueeze(1)  # Gating values for each token\n",
        "\n",
        "            # Gathering expert outputs using advanced indexing\n",
        "            expert_outputs = torch.stack([self.experts[expert_idx[j]](x_flat[j].unsqueeze(0)) for j in range(x_flat.shape[0])], dim=0).squeeze(1)\n",
        "\n",
        "            # Weighted sum of expert outputs\n",
        "            output += expert_outputs * expert_weight\n",
        "\n",
        "        # Reshaping back to (batch_size, seq_len, dim)\n",
        "        output = output.view(batch_size, seq_len, dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "class CustomOutput(nn.Module):\n",
        "    \"\"\"Custom Class instead of output layer when replacing the Linear layer with the MoE\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CustomOutput, self).__init__()\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # Bypassing transformation, but keeping residual connection\n",
        "        return hidden_states + input_tensor  # Residual connection remains intact\n",
        "\n",
        "\n",
        "def model_creation(num_classes=10):\n",
        "    \"\"\"Function to implement the ViT with MoE\"\"\"\n",
        "\n",
        "    # Loading Google ViT from Hugging Face\n",
        "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "    print(\"\\ndense model:\", model)\n",
        "\n",
        "    # Replacing Intermediate Linear Layers with Custom Class MoE in the last two transformer block\n",
        "    for n, layer in enumerate(model.vit.encoder.layer[-2:]):\n",
        "\n",
        "        intermediate_size = layer.intermediate.dense.out_features # Get the size of the intermediate layer\n",
        "        hidden_size = intermediate_size // 2\n",
        "        dim = layer.intermediate.dense.weight.shape[1]  # Input size: 768\n",
        "        hidden_dim = layer.intermediate.dense.weight.shape[0]\n",
        "        moe_layer = SparseMoE(dim, hidden_dim, num_experts=8, top_k=2)\n",
        "        layer.intermediate = moe_layer\n",
        "        layer.output = CustomOutput()\n",
        "\n",
        "    # Replacing the output of the ViT with the number of classes of our dataset\n",
        "    model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
        "\n",
        "    print(\"\\nMoE model:\", model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class for Custom Preprocessing of the dataset"
      ],
      "metadata": {
        "id": "wOXAJrLUx0KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset class for customized processing and transformation of inputs\"\"\"\n",
        "\n",
        "    def __init__(self, ds, transform=None, target_transform=None):\n",
        "        self.ds = ds\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.ds[idx]['img']\n",
        "        label = self.ds[idx]['label']\n",
        "        if image.shape[0] == 1:\n",
        "            image = image.repeat(3, 1, 1)\n",
        "\n",
        "        if self.transform:\n",
        "            inputs = self.transform(images=image, return_tensors=\"pt\")\n",
        "            pixel_values = inputs.pixel_values\n",
        "        if self.target_transform:\n",
        "            label = torch.tensor(label).clone().detach()\n",
        "        return inputs.pixel_values, label"
      ],
      "metadata": {
        "id": "khMKdmBcx1v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes and Functions to Plot and Analyze performance"
      ],
      "metadata": {
        "id": "rKOTflz_x3gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Class to easily computing and storing the average and current value\n",
        "    class taken from https://github.com/pranoyr/cnn-lstm/blob/7062a1214ca0dbb5ba07d8405f9fbcd133b1575e/utils.py#L52\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count  #\n",
        "\n",
        "def plot_performances(epoch, performances, model_name):\n",
        "    epochs = range(1, epoch + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
        "\n",
        "    # Set the font size for various elements\n",
        "    fontsize = 14  # Adjust this value as needed\n",
        "\n",
        "    # First plot (losses)\n",
        "    ax1.xaxis.set_label_coords(0.1, -0.2)\n",
        "    ax1.plot(np.array(epochs) - 0.5, performances['train_loss'], 'g', label='Training loss')\n",
        "    ax1.plot(np.array(epochs) - 0.5, performances['eval_loss'], 'b', label='Validation loss')\n",
        "    ax1.set_title('Training and Validation loss', fontsize=fontsize)\n",
        "    ax1.set_xlabel('Epochs', fontsize=fontsize)\n",
        "    ax1.set_ylabel('Loss', fontsize=fontsize)\n",
        "    ax1.set_xticks(np.arange(0.5, epoch + 0.5, 1))\n",
        "    ax1.set_xticklabels([str(int(tick)) for tick in range(1, epoch + 1, 1)], fontsize=fontsize)\n",
        "    ax1.legend(fontsize=fontsize)\n",
        "\n",
        "    # Second plot (accuracies)\n",
        "    ax2.plot(np.array(epochs) - 0.5, performances['train_acc'], 'g', label='Training accuracy')\n",
        "    ax2.plot(np.array(epochs) - 0.5, performances['eval_acc'], 'b', label='Validation accuracy')\n",
        "    ax2.set_title('Training and Validation accuracy', fontsize=fontsize)\n",
        "    ax2.set_xlabel('Epochs', fontsize=fontsize)\n",
        "    ax2.set_ylabel('Loss', fontsize=fontsize)\n",
        "    ax2.set_xticks(np.arange(0.5, epoch + 0.5, 1))\n",
        "    ax2.set_xticklabels([str(int(tick)) for tick in range(1, epoch + 1, 1)], fontsize=fontsize)\n",
        "    ax2.set_ylim(max(0, min(min(min(performances['train_acc']), min(performances['eval_acc'])) - 0.05, 1)))\n",
        "    ax2.legend(fontsize=fontsize)\n",
        "\n",
        "    # Adjust layout and save figure\n",
        "    plt.tight_layout()\n",
        "    fig_dir = os.path.join(os.getcwd(), 'figures', model_name + '_training.png')\n",
        "    os.makedirs(os.path.dirname(fig_dir), exist_ok=True)\n",
        "    plt.savefig(fig_dir)\n",
        "    plt.close('all')\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def calculate_performance(confusion_matrix, class_names, model_name):\n",
        "    \"\"\"Function to compute the model's performance on the test set\"\"\"\n",
        "\n",
        "    # Inizializing the confusion matrix\n",
        "    confusion_matrix = confusion_matrix\n",
        "    total_predicted = confusion_matrix.sum(0)\n",
        "    total_actual = confusion_matrix.sum(1)\n",
        "    totals = confusion_matrix.sum(1).sum(0)\n",
        "\n",
        "    df = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n",
        "    df_pred = pd.DataFrame(total_predicted, index=class_names)\n",
        "    df_actual = pd.DataFrame(total_actual, index=class_names)\n",
        "\n",
        "    total_acc = 0\n",
        "    recall = pd.DataFrame(np.zeros(len(class_names)), index=class_names)\n",
        "    precision = pd.DataFrame(np.zeros(len(class_names)), index=class_names)\n",
        "    overall_perf = pd.DataFrame(np.zeros(1), index=['accuracy'])\n",
        "    for name in class_names:\n",
        "        total_acc += df.loc[name, name]\n",
        "        recall.loc[name] = df.loc[name, name] / df_actual.loc[name]\n",
        "        precision.loc[name] = df.loc[name, name] / df_pred.loc[name]\n",
        "\n",
        "    accuracy = total_acc / totals * 100\n",
        "    error_rate = 100 - accuracy\n",
        "    overall_perf.loc['accuracy', 0] = accuracy\n",
        "\n",
        "    performance_test = {'accuracy': accuracy, 'error_rate': error_rate, 'recall': recall,\n",
        "                        'precision': precision, 'overall_acc': overall_perf}\n",
        "\n",
        "    print(f'RESULTS OF TEST {model_name}')\n",
        "    print(\"the accuracy of the model is {} %\".format(performance_test['accuracy']))\n",
        "    print(\"the error rate of the model is {} %\".format(performance_test['error_rate']))\n",
        "    print(\"recall : \\n{}\".format(performance_test['recall']))\n",
        "    print(\"precision : \\n{}\".format(performance_test['precision']))\n",
        "\n",
        "    results_dir = os.path.join('/content/drive/MyDrive/indeep/results', f'results_{model_name}.pkl')\n",
        "    confusion_matrix_dir = os.path.join('/content/drive/MyDrive/indeep/results', f'conf_matrix_{model_name}.pkl')\n",
        "    with open(results_dir, 'wb') as file:\n",
        "        pickle.dump(performance_test, file)\n",
        "    with open(confusion_matrix_dir, 'wb') as file:\n",
        "        pickle.dump(confusion_matrix, file)\n",
        "\n",
        "    return performance_test\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(performance_test, confusion_matrix, class_names):\n",
        "    fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(nrows=2, ncols=2,\n",
        "                                                 gridspec_kw={'width_ratios': [7, 3], 'height_ratios': [8, 2]})\n",
        "\n",
        "    # plot 1 confusion matrix\n",
        "    im = ax1.imshow(confusion_matrix)  # x axis = real class, y axis = predicted class\n",
        "    ax1.set_xticks(np.arange(len(class_names)))\n",
        "    ax1.set_yticks(np.arange(len(class_names)))\n",
        "    ax1.set_ylabel('Real Class', fontweight='bold', fontsize=9)\n",
        "    ax1.set_xlabel('Predicted Class', fontweight='bold', fontsize=9)\n",
        "\n",
        "    # Position the x-axis label at the top\n",
        "    ax1.xaxis.set_label_coords(.5, 1.15)  # Keep this for label positioning\n",
        "    ax1.xaxis.tick_top()  # Place ticks at the top\n",
        "    ax1.xaxis.set_label_position('top')  # Ensure the label is at the top\n",
        "\n",
        "    # Align the x-tick labels properly; \"right\" aligns the text to the right of the tick\n",
        "    ax1.tick_params(axis='x', which='both', top=True, labeltop=True, labelsize=10, pad=10)  # Adjust padding\n",
        "    ax1.tick_params(axis='y', which='both', labelsize=10)\n",
        "    ax1.set_xticklabels(class_names, rotation=30, ha=\"left\")\n",
        "    ax1.set_yticklabels(class_names, rotation=60, ha=\"right\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            text = ax1.text(j, i, int(confusion_matrix[i, j]),\n",
        "                            ha=\"center\", va=\"center\", color=\"w\")\n",
        "    ## plot 2 recall\n",
        "    ax2.set_title(\"Recall\", fontweight ='bold', fontsize=12)\n",
        "    im = ax2.imshow(performance_test['recall'])\n",
        "    for i, c in enumerate(class_names):\n",
        "        text = ax2.text(0, i, str(round(performance_test['recall'].iloc[i][0]*100, 1)), ha=\"center\", va=\"center\", color=\"w\")\n",
        "    ax2.tick_params(top=False, bottom=False, left=False, right=False)\n",
        "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
        "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
        "\n",
        "    ## plot 3 precision\n",
        "    ax3.set_title(\"Precision\", fontweight ='bold', fontsize=12)\n",
        "    im = ax3.imshow(performance_test['precision'].T)\n",
        "    for i, c in enumerate(class_names):\n",
        "        text = ax3.text(i, 0, str(round(performance_test['precision'].iloc[i][0]*100, 1)), ha=\"center\", va=\"center\",\n",
        "                        color=\"w\")\n",
        "    ax3.tick_params(top=False, bottom=False, left=False, right=False)\n",
        "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
        "    plt.setp(ax3.get_yticklabels(), visible=False)\n",
        "\n",
        "    # plot 4 macro average F1-score\n",
        "    ax4.set_title(\"Macro Avg F1-score\", fontweight ='bold', fontsize=12)\n",
        "    f1_scores = 2 * (performance_test['precision'].values * performance_test['recall'].values) / (\n",
        "                performance_test['precision'].values + performance_test['recall'].values)\n",
        "    macro_avg_f1 = f1_scores.mean()\n",
        "    print(macro_avg_f1)\n",
        "    im = ax4.imshow(pd.DataFrame([macro_avg_f1], columns=['Value']))\n",
        "    text = ax4.text(0, 0, str(round(macro_avg_f1*100, 2)), ha=\"center\", va=\"center\",\n",
        "                    color=\"w\")\n",
        "\n",
        "    ax4.tick_params(top=False, bottom=False, left=False, right=False)\n",
        "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
        "    plt.setp(ax4.get_yticklabels(), visible=False)\n",
        "\n",
        "    fig_dir = os.path.join(os.getcwd(), 'figures', 'conf_mtrx.png')\n",
        "    os.makedirs(os.path.dirname(fig_dir), exist_ok=True)\n",
        "    plt.savefig(fig_dir)\n",
        "    plt.show()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "hSGNvCBYx4up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loading(dataset):\n",
        "    \"\"\"Function to load, filter, preprocess the dataset and create Dataloader\"\"\"\n",
        "\n",
        "    global batch_size\n",
        "    batch_size = 32\n",
        "\n",
        "    # To train the dataset on CIFAR10\n",
        "    if dataset == 'cifar10':\n",
        "        # Load the dataset from Hugging Face\n",
        "        cifar10_dataset = load_dataset('uoft-cs/cifar10').with_format('torch')\n",
        "        dataset_to_train = cifar10_dataset\n",
        "\n",
        "        global class_names\n",
        "        class_names = cifar10_dataset['train'].features['label'].names\n",
        "        df_total = dataset_to_train['train'].to_pandas()\n",
        "\n",
        "        # for a smaller train set\n",
        "        #df_total = pd.concat([group.sample(frac=0.2) for _, group in df_total.groupby('label')], axis=0).reset_index(drop=True)\n",
        "\n",
        "        df_valid = pd.concat([group.sample(frac=0.2) for _, group in df_total.groupby('label')], axis=0).reset_index(drop=True)\n",
        "        df_train = df_total.loc[~df_total.index.isin(df_valid.index)].reset_index(drop=True)\n",
        "        dataset_to_train['train'] = Ds.from_pandas(df_train, features=dataset_to_train['train'].features).with_format('torch')\n",
        "        dataset_to_train['valid'] = Ds.from_pandas(df_valid, features=dataset_to_train['train'].features).with_format('torch')\n",
        "\n",
        "    # To train the dataset on tiny ImageNet !!! this dataset does not have any test set\n",
        "    elif dataset == 'tiny_imagenet':\n",
        "        # Load the dataset from Hugging Face\n",
        "        tiny_imagenet_dataset = load_dataset('Maysee/tiny-imagenet').with_format('torch')\n",
        "\n",
        "        # Creating a subset of the dataset for faster training\n",
        "        df_train = tiny_imagenet_dataset['train'].to_pandas()\n",
        "        df_train_filtered = pd.concat([group.sample(frac=0.2) for _, group in df_train[df_train['label'] < 20].groupby('label')], axis=0).reset_index(drop=True)\n",
        "        df_valid = tiny_imagenet_dataset['valid'].to_pandas()\n",
        "        df_valid_filtered = pd.concat([group.sample(frac=0.2) for _, group in df_valid[df_valid['label'] < 20].groupby('label')], axis=0).reset_index(drop=True)\n",
        "        filtered_ds_train = Ds.from_pandas(df_train_filtered, features=tiny_imagenet_dataset['train'].features).with_format('torch')\n",
        "        filtered_ds_valid = Ds.from_pandas(df_valid_filtered, features=tiny_imagenet_dataset['valid'].features).with_format('torch')\n",
        "        print(filtered_ds_train)\n",
        "        filtered_dataset = DatasetDict({\n",
        "            'train': filtered_ds_train,\n",
        "            'valid': filtered_ds_valid\n",
        "        })\n",
        "        dataset_to_train = filtered_dataset\n",
        "\n",
        "    # Loading the preprocessing rules directly from the model\n",
        "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "    # Preprocessing the data\n",
        "    dataset = {x: CustomImageDataset(dataset_to_train[x], transform=processor, target_transform=True) for x in\n",
        "               ['train', 'valid', 'test']}\n",
        "    # Creating the DataLoader for the training\n",
        "    shuffle_ds = {'train': True, 'valid': False, 'test': False}\n",
        "    dataloader = {x: DataLoader(dataset[x], batch_size=batch_size, shuffle=shuffle_ds[x]) for x in ['train', 'valid', 'test']}\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "U0Tma4KgyTey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(dataloader, model):\n",
        "    \"\"\"Function to train the model\"\"\"\n",
        "\n",
        "    # Defining Training Params\n",
        "    num_epochs = 4\n",
        "    learning_rate = 2e-5\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # Currently not used for the training because num_epochs = step_size\n",
        "    training_results = {'valid_accuracy': [], 'total_loss': []}\n",
        "    last_loss = 100\n",
        "\n",
        "    # parameters for early stopping (not used in the current training because num_epochs = patience)\n",
        "    patience = 5\n",
        "    trigger_times = 0\n",
        "\n",
        "    performances = {\n",
        "        'best_acc': 0,\n",
        "        'train_acc': [],\n",
        "        'train_loss': [],\n",
        "        'eval_loss': [],\n",
        "        'eval_acc': [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        for phase in ['train', 'valid']:\n",
        "\n",
        "            if phase == 'train':\n",
        "                print(f'Number of batches in train_loader: {len(dataloader[phase])}')\n",
        "                current_batch = dataloader[phase].batch_size\n",
        "                print(f'Batch size: {current_batch}')\n",
        "                print(f'Number of samples in dataset: {len(dataloader[phase].dataset)}')\n",
        "                train_features, train_labels = next(iter(dataloader[phase]))\n",
        "                print(f'Feature batch shape: {train_features.squeeze(1).shape}')  # IF IT STOPS...\n",
        "                print(f'Labels batch shape: {train_labels.shape}')\n",
        "                model.train()\n",
        "            else:\n",
        "                print(f'\\nValidation phase is starting at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "                model.eval()\n",
        "\n",
        "            losses = AverageMeter()\n",
        "            accuracies = AverageMeter()\n",
        "\n",
        "            for batch_idx, batch_data in enumerate(dataloader[phase]):\n",
        "                batch = {'pixel_values': batch_data[0], 'label': batch_data[1]}\n",
        "                images, labels = batch['pixel_values'].squeeze(1).to(device), batch['label'].to(device)\n",
        "                correct, total = 0, 0\n",
        "                if phase == 'train':\n",
        "                    print(f'training status {round((batch_idx+(len(dataloader[phase])*(epoch-1)))/(len(dataloader[phase])*num_epochs)*100, 2)}% --> batch n: {batch_idx + 1} at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(images).logits\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Calculating loss and backpropagate\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                batch_accuracy = torch.sum(predicted == labels) / current_batch\n",
        "                losses.update(loss.item(), current_batch)\n",
        "                accuracies.update(batch_accuracy.item(), current_batch)\n",
        "\n",
        "            print(f'Epoch [{epoch}/{num_epochs}] {phase} set ({len(dataloader[phase].dataset)} samples): Average loss: {losses.avg:.4f}\\tAcc: {accuracies.avg * 100:.4f}%')\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                performances['train_loss'].append(losses.avg)\n",
        "                performances['train_acc'].append(accuracies.avg)\n",
        "            else:\n",
        "                performances['eval_loss'].append(losses.avg)\n",
        "                performances['eval_acc'].append(accuracies.avg)\n",
        "                if accuracies.avg > performances['best_acc']:\n",
        "                    performances['best_acc'] = accuracies.avg\n",
        "\n",
        "        # Plotting Learning curves\n",
        "        plot_performances(epoch, performances, model_name)\n",
        "\n",
        "        # Custom Early Stopping Rule\n",
        "        if losses.avg > last_loss:\n",
        "            trigger_times += 1\n",
        "            print('Trigger Times before patience:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print(\"Epoch {}\\n\".format(epoch))\n",
        "                print('Early stopping!')\n",
        "\n",
        "                checkpoint_dict = {'epoch': epoch,\n",
        "                              # The current epoch number, which helps resume training from the correct point.\n",
        "                              'state_dict': model.state_dict(),\n",
        "                              # The model's weights (parameters), stored in the state_dict format.\n",
        "                              'optimizer_state_dict': optimizer.state_dict(),\n",
        "                              # The state of the optimizer, including momenta and other parameters.\n",
        "                              'loss_function': criterion,  # The loss function (criterion) used during training.\n",
        "                              'performances': performances,  # Example of additional metadata\n",
        "                              'lr_scheduler_state_dict': scheduler.state_dict(),  # If using a scheduler\n",
        "                              'hyperparameters': {\n",
        "                                  'batch_size': batch_size,\n",
        "                                  'learning_rate': learning_rate\n",
        "                              },\n",
        "                              'random_seed': random_seed,\n",
        "                              'library_versions': {\n",
        "                                  'torch': torch.__version__,\n",
        "                                  'numpy': np.__version__\n",
        "                              }\n",
        "                              }\n",
        "                checkpoint_dir = os.path.join('/content/drive/MyDrive/indeep/checkpoints', f'checkpoint_{model_name}_{num_epochs}_epoch.pth')\n",
        "                print(f'\\nsaving checkpoint in {checkpoint_dir}\\n')\n",
        "                torch.save(checkpoint_dict, checkpoint_dir)\n",
        "\n",
        "                return performances, model\n",
        "\n",
        "\n",
        "        last_loss = losses.avg\n",
        "        checkpoint_dict = {'epoch': epoch,\n",
        "                      # The current epoch number, which helps resume training from the correct point.\n",
        "                      'state_dict': model.state_dict(),\n",
        "                      # The model's weights (parameters), stored in the state_dict format.\n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      # The state of the optimizer, including momenta and other parameters.\n",
        "                      'loss_function': criterion,  # The loss function (criterion) used during training.\n",
        "                      'performances': performances,  # Performances of the model\n",
        "                      'lr_scheduler_state_dict': None,  # If using a scheduler\n",
        "                      'hyperparameters': {\n",
        "                          'batch_size': batch_size,\n",
        "                          'learning_rate': learning_rate\n",
        "                      },\n",
        "                      'random_seed': random_seed,\n",
        "                      'library_versions': {\n",
        "                          'torch': torch.__version__,\n",
        "                          'numpy': np.__version__\n",
        "                      }\n",
        "                      }\n",
        "        checkpoint_dir = os.path.join('/content/drive/MyDrive/indeep/checkpoints', f'checkpoint_{model_name}_{num_epochs}_epoch.pth')\n",
        "        print(f'\\nsaving checkpoint in {checkpoint_dir}\\n')\n",
        "        torch.save(checkpoint_dict, checkpoint_dir)\n",
        "\n",
        "    return performances, model"
      ],
      "metadata": {
        "id": "c0AickSkyJzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model):\n",
        "    \"\"\"Function to compute the model performance on the test set\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    confusion_matrix = np.zeros((len(class_names), len(class_names)))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(dataloader['test']):\n",
        "            batch = {'pixel_values': batch_data[0], 'label': batch_data[1]}\n",
        "            images, labels = batch['pixel_values'].squeeze(1).to(device), batch['label'].to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            for i, l in enumerate(labels):\n",
        "                all_labels.append(labels[i].item())\n",
        "                all_predictions.append(predicted[i].item())\n",
        "\n",
        "                confusion_matrix[int(labels[i]), int(predicted[i])] += 1\n",
        "\n",
        "        results = {'label': all_labels, 'predictions': all_predictions}\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        performance_test = calculate_performance(confusion_matrix, class_names, model_name)\n",
        "\n",
        "        plot_confusion_matrix(performance_test, confusion_matrix, class_names)\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "ig63GlPRyd5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can actually start the training, using all the functions and classes defined above"
      ],
      "metadata": {
        "id": "XxM9oJt2JiXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset and creating the dataloader\n",
        "dataloader = data_loading(dataset=dataset_name)"
      ],
      "metadata": {
        "id": "ZQZguhMpykAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "model = model_creation(num_classes=len(class_names))"
      ],
      "metadata": {
        "id": "XnlTgQVnyiQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e30f8ac-44ad-48d0-e187-d9e6ef1a99f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dense model: ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTSdpaAttention(\n",
            "            (attention): ViTSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n",
            "\n",
            "MoE model: ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-9): 10 x ViTLayer(\n",
            "          (attention): ViTSdpaAttention(\n",
            "            (attention): ViTSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (10-11): 2 x ViTLayer(\n",
            "          (attention): ViTSdpaAttention(\n",
            "            (attention): ViTSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): SparseMoE(\n",
            "            (experts): ModuleList(\n",
            "              (0-7): 8 x Sequential(\n",
            "                (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              )\n",
            "            )\n",
            "            (gate): Linear(in_features=768, out_features=8, bias=True)\n",
            "          )\n",
            "          (output): CustomOutput()\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the training\n",
        "model.to(device)\n",
        "performances, model = training(dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "hwlKRXREylbH",
        "outputId": "c9bbf4a2-100c-4081-cba6-3916da69f100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Number of batches in train_loader: 1250\n",
            "Batch size: 32\n",
            "Number of samples in dataset: 40000\n",
            "Feature batch shape: torch.Size([32, 3, 224, 224])\n",
            "Labels batch shape: torch.Size([32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-07f611e2ed25>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).clone().detach()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training status 0.0% --> batch n: 1 at 2024-11-30 11:18:22\n",
            "training status 0.02% --> batch n: 2 at 2024-11-30 11:18:33\n",
            "training status 0.04% --> batch n: 3 at 2024-11-30 11:18:45\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3baeb01d5fdb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Starting the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mperformances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-91eb10fdc35a>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         outputs = self.vit(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[1;32m    467\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# in ViT, layernorm is also applied after self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# second residual connection is done here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-623d1b480f59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Gathering expert outputs using advanced indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mexpert_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Weighted sum of expert outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-623d1b480f59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Gathering expert outputs using advanced indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mexpert_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Weighted sum of expert outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training is concluded we can assess the actual performance on a test set. If you have just concluded the training you can skip the loading of the checkpoint.\n",
        "Otherwise, if you want only to test the model, build the dataloader, build the model again, define the function needed, load the checkpoint, and go to the testing!"
      ],
      "metadata": {
        "id": "R7KxjGCZKhGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'checkpoint_{model_name}_{num_epochs}_epoch.pth'\n",
        "checkpoint_dir = os.path.join('/content/drive/MyDrive/indeep/checkpoints', filename)\n",
        "checkpoint = torch.load(checkpoint_dir, map_location=torch.device(device))"
      ],
      "metadata": {
        "id": "ebrau63PKg0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "b10eb112-2e43-455a-9e21-395f08e24c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-6dbc8dc9c890>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_dir, map_location=torch.device(device))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/indeep/checkpoints/checkpoint_MoE_ViT_on_cifar10_5_epoch.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-6dbc8dc9c890>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint_MoE_ViT_on_cifar10_5_epoch.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/indeep/checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/indeep/checkpoints/checkpoint_MoE_ViT_on_cifar10_5_epoch.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "test(dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "jivQdihHyo_T",
        "outputId": "1c079414-a95e-4835-f509-91d1f476ffd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3f5ee4297226>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ]
    }
  ]
}